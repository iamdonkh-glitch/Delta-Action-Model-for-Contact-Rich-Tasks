# ================================================================
# record_peginsert_demo.py â€” Zero-action rollout (Hydra-based)
# ================================================================
# ç›®çš„ï¼šåŠ è½½ Factory PegInsert ç¯å¢ƒå¹¶æ‰§è¡Œç¨³å®šçš„é›¶åŠ¨ä½œæ§åˆ¶ï¼Œ
# è®©æœºæ¢°è‡‚åˆå§‹åŒ–åˆ°å½“å‰å§¿æ€å¹¶ä¿æŒé™æ­¢ï¼ˆä¸æŠ½æã€ä¸æ—‹è½¬ï¼‰

# ./isaaclab.sh -p custom/record_peginsert_demo.py \
#   --task Isaac-Factory-PegInsert-Direct-v0 \
#   --num_envs 1 --device cuda

import argparse
import sys
import time
from distutils.util import strtobool
import torch

from isaaclab.app import AppLauncher

# -------------------------------------------------------------
# CLI
# -------------------------------------------------------------
parser = argparse.ArgumentParser(description="Zero-action rollout for Factory PegInsert task.")
parser.add_argument("--task", type=str, default="Isaac-Factory-PegInsert-Direct-v0")
parser.add_argument("--agent", type=str, default="rl_games_cfg_entry_point")
parser.add_argument("--num_envs", type=int, default=1)
parser.add_argument("--max_steps", type=int, default=500)
parser.add_argument("--sleep", type=float, default=0.02)
# âš ï¸ ä¸æ·»åŠ  --deviceï¼›AppLauncher ä¼šè‡ªåŠ¨æ·»åŠ 

# æ·»åŠ  AppLauncher å‚æ•°
AppLauncher.add_app_launcher_args(parser)

# è§£æ CLI
args_cli, hydra_args = parser.parse_known_args()
sys.argv = [sys.argv[0]] + hydra_args

# å¯åŠ¨ Omniverse App
app_launcher = AppLauncher(args_cli)
simulation_app = app_launcher.app

# -------------------------------------------------------------
# å…¶ä½™å¯¼å…¥
# -------------------------------------------------------------
import gymnasium as gym
import omni
from isaaclab_tasks.utils.hydra import hydra_task_config
from isaaclab.envs import ManagerBasedRLEnvCfg, DirectRLEnvCfg
from isaaclab_rl.rl_games import RlGamesVecEnvWrapper
import isaaclab_tasks  # noqa: F401


# -------------------------------------------------------------
# è¾…åŠ©å‡½æ•°ï¼šå…¼å®¹ä¸åŒ Gym API
# -------------------------------------------------------------
def step_compat(env, action):
    """å…¼å®¹ Gymnasium / IsaacLab step è¿”å›å€¼"""
    try:
        return env.step(action)
    except ValueError:
        obs, rew, done, info = env.step(action)
        term, trunc = done, done
        return obs, rew, term, trunc, info


# -------------------------------------------------------------
# ç¨³å®šåˆå§‹åŒ–å‡½æ•°
# -------------------------------------------------------------
def stabilize_zero_action(env, device):
    """è®©å½“å‰æœºæ¢°è‡‚å§¿æ€æˆä¸º zero-action çš„ç¨³æ€ä½å§¿"""
    try:
        robot = env.unwrapped.scene["robot"]

        # 1ï¸âƒ£ è®©ç‰©ç†ç³»ç»Ÿç¨³å®šå‡ æ­¥
        for _ in range(5):
            env.unwrapped.sim.step(render=False)

        # 2ï¸âƒ£ è¯»å–å½“å‰å…³èŠ‚è§’ä¸é€Ÿåº¦
        qpos = robot.data.joint_pos.clone()
        qvel = torch.zeros_like(robot.data.joint_vel)

        # 3ï¸âƒ£ å†™å›å¹¶å›ºå®šå½“å‰çŠ¶æ€
        robot.write_joint_state(qpos, qvel)
        env.unwrapped.sim.step(render=False)

        # 4ï¸âƒ£ æ›´æ–°æ§åˆ¶å™¨é»˜è®¤ç›®æ ‡ï¼Œä½¿ zero-action = å½“å‰å§¿æ€
        if hasattr(env.unwrapped, "controllers") and "arm_action" in env.unwrapped.controllers:
            ctrl = env.unwrapped.controllers["arm_action"]
            if hasattr(ctrl, "set_default_target"):
                ctrl.set_default_target(qpos)
                print("[INIT] Controller default target set to current joint pose.")

        # 5ï¸âƒ£ æ¸…ç©ºåŠ¨ä½œç¼“å­˜
        if hasattr(env.unwrapped, "managers") and "action_manager" in env.unwrapped.managers:
            amgr = env.unwrapped.managers["action_manager"]
            if hasattr(amgr, "reset"):
                amgr.reset()
                print("[INIT] Action manager reset for clean start.")

        print("[INIT] Robot stabilized; zero-action now matches current pose.")

    except Exception as e:
        print("[WARN] Failed to align zero-action with current pose:", e)


# -------------------------------------------------------------
# ä¸»å‡½æ•°ï¼ˆHydra è£…é¥°å™¨ï¼‰
# -------------------------------------------------------------
@hydra_task_config(args_cli.task, args_cli.agent)
def main(env_cfg: ManagerBasedRLEnvCfg | DirectRLEnvCfg, agent_cfg: dict):
    print(f"[INFO] Launching {args_cli.task} with Hydra.")
    env_cfg.scene.num_envs = args_cli.num_envs
    env_cfg.sim.device = args_cli.device

    # =========================================================
    # âš™ï¸ ç¨³å®šæ€§é˜²æŠ½æè¡¥ä¸
    # =========================================================
    try:
        env_cfg.actions.arm_action.use_default_offset = False
        print("[PATCH] use_default_offset=False")
    except Exception:
        pass

    try:
        env_cfg.actions.arm_action.scale = 0.2
        print("[PATCH] arm_action.scale=0.2")
    except Exception:
        pass

    try:
        env_cfg.randomization.reset_on_init = False
        print("[PATCH] randomization.reset_on_init=False")
    except Exception:
        pass

    try:
        physx = env_cfg.sim.physx
        physx.solver_position_iteration_count = max(physx.solver_position_iteration_count, 16)
        physx.solver_velocity_iteration_count = max(physx.solver_velocity_iteration_count, 1)
        physx.contact_offset = 0.005
        physx.rest_offset = 0.0
        if hasattr(physx, "enable_stabilization"):
            physx.enable_stabilization = True
        print("[PATCH] Increased PhysX stability parameters.")
    except Exception:
        pass

    LOCK_WRIST_ROT_IN_LOOP = True

    # =========================================================
    # ğŸ§© åˆ›å»ºç¯å¢ƒï¼ˆä¸ train.py ä¸€è‡´ï¼‰
    # =========================================================
    env = gym.make(args_cli.task, cfg=env_cfg)
    try:
        env = RlGamesVecEnvWrapper(env, env_cfg.sim.device,
                                   clip_obs=float("inf"), clip_actions=float("inf"))
    except TypeError:
        env = RlGamesVecEnvWrapper(env, env_cfg.sim.device,
                                   clip_obs=float("inf"), clip_actions=float("inf"),
                                   obs_groups=None)

    obs, _ = env.reset()

    # =========================================================
    # ğŸ¦¾ åˆå§‹åŒ–æœºæ¢°è‡‚åˆ°ç¨³å®šå§¿æ€
    # =========================================================
    stabilize_zero_action(env, args_cli.device)

    print("[INFO] Starting zero-action rollout (Ctrl+C to exit).")
    step_i = 0
    while simulation_app.is_running() and step_i < args_cli.max_steps:
        action = torch.zeros((env.unwrapped.num_envs,
                              env.unwrapped.action_space.shape[-1]),
                             device=args_cli.device)
        

        # é” wrist æ—‹è½¬ï¼ˆé¿å…é«˜é€ŸæŠ½æï¼‰
        #if LOCK_WRIST_ROT_IN_LOOP and action.shape[-1] >= 6:
            #action[..., 3:6] = 0.0

        action = torch.clamp(action, -1.0, 1.0)
        obs, rew, term, info = step_compat(env, action)

        if step_i % 10 == 0:
            mean_r = torch.mean(torch.tensor(rew)).item()
            print(f"[Step {step_i}] mean_reward={mean_r:.4f}")

        step_i += 1
        time.sleep(args_cli.sleep)

    env.close()
    print("[INFO] Rollout finished.")


if __name__ == "__main__":
    main()
    simulation_app.close()
